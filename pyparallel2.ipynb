{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Add all necessary imports here\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.reload_library()\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<image>\n",
    "<section>\n",
    "<div>\n",
    "    <div>\n",
    "        <p> &nbsp;</p>\n",
    "        <h1>Parallel computing in <strong>Python</strong> </h1>\n",
    "         <p>\n",
    "             <strong><span>Vamshidhar Gangu</span></strong>\n",
    "         </p>\n",
    "         <p>\n",
    "             <span> HPC specialist</span> \n",
    "         </p>\n",
    "         <p>&nbsp;</p>\n",
    "         <p>&nbsp;</p>\n",
    "</div>\n",
    "</section>\n",
    "</image>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python GIL\n",
    " \n",
    "* Global Interpreter Lock\n",
    "* default Python is designed with simplicity in mind, so they made it thread-safe (GIL)\n",
    "* Restrict python to run in a single thread\n",
    "* __exectues only one statement at a time (serial processing or single-threading)__\n",
    "* Cannot make use of data stored in shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "![pGIL](img/pGIL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python GIL problem\n",
    "\n",
    "_**Factorial example using Threading**_\n",
    "\n",
    "``` python\n",
    "from datetime import datetime\n",
    "import threading \n",
    "\n",
    "def factorial(number): \n",
    "    fact = 1\n",
    "    for n in range(1, number+1): \n",
    "        fact *= n \n",
    "    return fact \n",
    "\n",
    "number = 100000 \n",
    "thread = threading.Thread(target=factorial, args=(number,)) \n",
    "startTime = datetime.now() \n",
    "thread.start() \n",
    "thread.join() \n",
    "endTime = datetime.now() \n",
    "print \"Time for execution: \", endTime - startTime\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "run time:\n",
    "    * 1 Thread  : 3.4 sec\n",
    "    * 2 Threads : 6.2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You donâ€™t get the concurrency needed with Python multithreading because of the Global interpreter lock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# multi-threading vs. multi-processing\n",
    "\n",
    "### multi-threading\n",
    "* jobs pictured as \"sub-tasks\" of a single process \n",
    "* have access to the same memory (shared memory)\n",
    "* can lead conflicts (improper synchronization) \n",
    "    * _writing to same memory location at the same time_\n",
    "\n",
    "### multi-processing\n",
    "* safer approach (although has communication overhead)\n",
    "* each process is completed independently from each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel python on HPC\n",
    "\n",
    "1. Processors\n",
    "2. Nodes\n",
    "3. Internodes\n",
    "![HPC](img/HPC.PNG)\n",
    "\n",
    "**Parallel8**\n",
    "```\n",
    "#PBS -q parallel8\n",
    "#PBS -l select=1:ncpus=8:mpiprocs=8:mem=10GB\n",
    "```\n",
    "**Parallel12**\n",
    "```\n",
    "#PBS -q parallel12\n",
    "#PBS -l select=1:ncpus=12:mpiprocs=12:mem=10GB\n",
    "```\n",
    "**Parallel24**\n",
    "```\n",
    "#PBS -q parallel24\n",
    "#PBS -l select=1:ncpus=24:mpiprocs=24:mem=10GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Map function\n",
    "Used to run a function over multiple elements\n",
    "``` python\n",
    "def square(a):\n",
    "    return a*a\n",
    "outputs =[]\n",
    "for i in inputs:\n",
    "    outpus.append(square(i))\n",
    "# or\n",
    "outputs = [square(i) for i in inputs]\n",
    "#or\n",
    "outputs = map(f, inputs)\n",
    "```\n",
    "![mapfn](img/map_fn.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel frameworks \n",
    "\n",
    "\n",
    "- [multiprocessing](https://docs.python.org/2/library/multiprocessing.html)\n",
    "\n",
    "- [__*concurrent.futures*__](https://docs.python.org/3/library/concurrent.futures.html)\n",
    "\n",
    "- [__*joblib*__](https://pythonhosted.org/joblib/)\n",
    "\n",
    "- [ipyparallel](https://ipyparallel.readthedocs.io/en/latest/)\n",
    "\n",
    "- [MPI4py](http://mpi4py.readthedocs.io/en/stable/)\n",
    "\n",
    "- [__*Dask*__](https://dask.pydata.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# futures (concurrent.futures)\n",
    "* part of standard library (python 3.2)\n",
    "* abstract layer on top of Python's threading and multiprocessing modules\n",
    "\n",
    "* **`executor`**\n",
    "    * abstract class (can not be used directly)\n",
    "    * *`ThreadPoolExecutor`* :- multithreading\n",
    "    * *`ProcessPoolExecutor`* :- multiprocessing\n",
    "    * submit multiple tasks to `Pool`\n",
    "    * `Pool` assign tasks and schedule them to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# futures: sum of all primes below *n*\n",
    "\n",
    "``` python\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def is_prime(num):\n",
    "    if num <= 1:\n",
    "        return False\n",
    "    elif num <= 3:\n",
    "        return True\n",
    "    elif num%2 == 0 or num%3 == 0:\n",
    "        return False\n",
    "    i = 5\n",
    "    while i*i <= num:\n",
    "        if num%i == 0 or num%(i+2) == 0:\n",
    "            return False\n",
    "        i += 6\n",
    "    return True\n",
    "\n",
    "\n",
    "def find_sum(num):\n",
    "    sum_of_primes = 0\n",
    "    ix = 2\n",
    "    while ix <= num:\n",
    "        if is_prime(ix):\n",
    "            sum_of_primes += ix\n",
    "        ix += 1\n",
    "    return sum_of_primes\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### multi threading\n",
    "\n",
    "``` python\n",
    "def sum_primes_thread(nums):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 4) as executor:\n",
    "        for number, sum_res in zip(nums, executor.map(find_sum, nums)):\n",
    "            print(\"{} : Sum = {}\".format(number, sum_res))\n",
    "```\n",
    "### multiprocessing\n",
    "\n",
    "``` python\n",
    "def sum_primes_process(nums):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = 4) as executor:\n",
    "        for number, sum_res in zip(nums, executor.map(find_sum, nums)):\n",
    "            print(\"{} : Sum = {}\".format(number, sum_res))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nums = [100000, 200000, 300000]\n",
    "    start = time.time()\n",
    "    sum_primes_thread(nums)\n",
    "    sum_primes_process(nums\n",
    "    print(\"Time taken = {0:.5f}\".format(time.time() - start))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Output when executing `sum_primes_process`\n",
    "\n",
    "`\n",
    "100000 : Sum = 454396537\n",
    "200000 : Sum = 1709600813\n",
    "300000 : Sum = 3709507114\n",
    "Time Taken = 0.71783\n",
    "`\n",
    "\n",
    "Output when executing `sum_primes_thread`\n",
    "\n",
    "`\n",
    "100000 : Sum = 454396537\n",
    "200000 : Sum = 1709600813\n",
    "300000 : Sum = 3709507114\n",
    "Time Taken = 1.2338\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## as_completed & wait\n",
    "\n",
    "#### as_completed()\n",
    "* yeilds results as soon as futures start resolving\n",
    "* vs `map()` : returns the results in order\n",
    "\n",
    "#### wait()\n",
    "* returns tuple with two sets\n",
    "* one with completed and other conatins the uncompleted one's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## as_completed & wait\n",
    "\n",
    "``` python\n",
    "from concurrent.futures import ThreadPoolExecutor, wait, as_completed\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "def return_after_5_secs(num):\n",
    "    sleep(randint(1, 5))\n",
    "    return \"Return of {}\".format(num)\n",
    "\n",
    "pool = ThreadPoolExecutor(5)\n",
    "futures = []\n",
    "for x in range(5):\n",
    "    futures.append(pool.submit(return_after_5_secs, x))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*`as_completed`*\n",
    "\n",
    "``` python\n",
    "for x in as_completed(futures):\n",
    "    print(x.result())\n",
    "```\n",
    "\n",
    "_`wait`_\n",
    "``` python\n",
    "print(wait(futures))\n",
    "```\n",
    "* `wait` controls : `return_when` : `FIRST_COMPLETED, FIRST_EXCEPTION`, `ALL_COMPLETED`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# joblib\n",
    "* another parallel processing library\n",
    "* developed by authors who work on *scikit-learn*\n",
    "* also built on top of multiprocessing, multithreading\n",
    "* ability to use a pool of worker like a context manager, reused across several tasks to be parallized\n",
    "* if `njobs` set to 1, then it is puerly sequential mode, no overhead of setting up a pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from math import sqrt\n",
    "Parallel(n_jobs=1)(delayed(sqrt) (i**2) for i in range(10))\n",
    "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# MPI4py\n",
    "\n",
    "* python binding for MPI (Message Passing Interface)\n",
    "* _distributed_ parallel programming in python\n",
    "* Based ob MPI-2 C++ bindings\n",
    "* Almost all MPI calls supported\n",
    "* API docs: http://pythonhosted.org/mpi4py/apiref/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Minimal mpi4py example\n",
    "\n",
    "``` python\n",
    "from mpi4py import MPI\n",
    "com = MPI.COMM_WORLD\n",
    "print(\"%d of %d\" %(comm.Get_rank(), comm.Get_size()))\n",
    "```\n",
    "\n",
    "Use **mpirun** and **python** to execute this script\n",
    "\n",
    "`$ mpirun -n 4 python script.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notes:\n",
    "* MPI_Init is called when mpi4py is imported\n",
    "* MPI_Finalize is called when the scipt exits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## P2P communication\n",
    "\n",
    "**send() and recv()**\n",
    "\n",
    "\n",
    "* one to one - one node to another.\n",
    "* one to many - one node to all nodes or many of them.\n",
    "* many to one - many nodes, or all nodes, to one node (usually the master).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "``` python\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "name = MPI.Get_processor_name()\n",
    "\n",
    "shared = (rank+1)*5\n",
    "\n",
    "if rank == 0:\n",
    "    data = shared\n",
    "    comm.send(data, dest=1)\n",
    "    comm.send(data, dest=2)\n",
    "    print 'From rank',name,'we sent',data\n",
    "    time.sleep(5)\n",
    "\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print 'on node',name, 'we received:',data\n",
    "\n",
    "\n",
    "elif rank == 2:\n",
    "    data = comm.recv(source=0)\n",
    "    print 'on node',name, 'we received:',data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Collective communication: Bcast\n",
    "\n",
    "Broadcasting data to all the nodes\n",
    "\n",
    "``` python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a':1,'b':2,'c':3}\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.bcast(data, root=0)\n",
    "print 'rank',rank,data\n",
    "```\n",
    "All the nodes have the same values for `data`\n",
    "\n",
    "`$mpirun -np 5 python bcast.py\n",
    "rank 0 {'a':1,'b':2,'c':3}\n",
    "rank 4 {'a':1,'b':2,'c':3}\n",
    "rank 1 {'a':1,'b':2,'c':3}\n",
    "rank 3 {'a':1,'b':2,'c':3}\n",
    "rank 2 {'a':1,'b':2,'c':3}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Collective communication: Scatter\n",
    "\n",
    "scatter the data elements around the processing nodes\n",
    "``` python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "   data = [(x+1)**x for x in range(size)]\n",
    "   print 'we will be scattering:',data\n",
    "else:\n",
    "   data = None\n",
    "   \n",
    "data = comm.scatter(data, root=0)\n",
    "print 'rank',rank,'has data:',data\n",
    "```\n",
    "Now we see elements of data is scattered among processors\n",
    "\n",
    "`$mpirun -np 5 python scatter.py\n",
    "we will be scattering: [1, 2, 9, 64, 625]\n",
    "rank 0 has data: 1\n",
    "rank 1 has data: 2\n",
    "rank 3 has data: 9\n",
    "rank 4 has data: 64\n",
    "rank 5 has data: 625\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Note*: can only scatter as many elements as you have processors, An error is raised, if you attempt to scatter more elements than your processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Collectives comm: Gather\n",
    "\n",
    "Opposite to Scatter, gathers all elements from the worker nodes on master node\n",
    "\n",
    "``` python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "   data = [(x+1)**x for x in range(size)]\n",
    "   print 'we will be scattering:',data\n",
    "else:\n",
    "   data = None\n",
    "   \n",
    "data = comm.scatter(data, root=0)\n",
    "data += 1\n",
    "print 'rank',rank,'has data:',data\n",
    "\n",
    "newData = comm.gather(data,root=0)\n",
    "\n",
    "if rank == 0:\n",
    "   print 'master:',newData\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Output \n",
    "\n",
    "`\n",
    "we will be scattering: [1, 2, 9, 64, 625]\n",
    "rank 0 has data: 1\n",
    "rank 2 has data: 9\n",
    "rank 4 has data: 625\n",
    "rank 3 has data: 64\n",
    "rank 1 has data: 2\n",
    "master collected: [2, 3, 10, 65, 626]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### MPI4py  communications\n",
    "\n",
    "P2 comm:\n",
    "\n",
    "* Send(data, dest, tag)\n",
    "* Recv(data, source, tag)\n",
    "* send/recv : general Python objects, **slow**\n",
    "* Send/Recv : continuous arrays, **fast**\n",
    "\n",
    "Collectives:\n",
    "\n",
    "* Bcast (Broadcast)\n",
    "* Scatter\n",
    "* Gather \n",
    "* Reduction \n",
    "\n",
    "Tutorial:\n",
    "http://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
    "\n",
    "MPI4py API reference\n",
    "http://mpi4py.scipy.org/docs/apiref/frames.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "![dask](img/dask-logo.png)\n",
    "\n",
    "\n",
    "* Provides advanced parallelism for analytics\n",
    "* That leverages the excellent python ecosystem \n",
    "   - Pandas, Numpy, Scikit-learn\n",
    "   - Multicore and distributed clusters\n",
    "* Using blocked algorithms and task scheduling\n",
    "* Written in pure python (pip install, conda install)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask Architectue\n",
    "\n",
    "- High Level collections\n",
    "- Dask graphs\n",
    "- Task schedulers\n",
    "\n",
    "\n",
    "![dask-arch](img/dask-arch2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dask.array\n",
    "![dask-arr](img/dask-array.PNG)\n",
    "\n",
    "Out-of-core, parallel, n-dimensional array library     \n",
    "* Compies the numpy interface\n",
    "   - Arithmetic, scalar math: `+, \\*, exp, log,..`\n",
    "   - Axes reductions: `sum(), mean(), std(), sum(axis=0),..`\n",
    "   - some linalg: `tensordot, qr, svd`\n",
    "   - Slicing `x[:100, 500:100:-2]`\n",
    " \n",
    "* New operations\n",
    "  - Parallel algo (`approximate quantiles, topk`,..)\n",
    "  - Integration with HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# example\n",
    "\n",
    "1. Construct a 20k x 20k array of normally distributed randome values broken up into 1000x1000 sized chunks\n",
    "2. Take the mean along one axis\n",
    "3. Take every 100th element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NumPy\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "%%time\n",
    "x = np.random.normal(10, 0.1, size=(20000,20000))\n",
    "y = x.mean(axis=0)[::100]\n",
    "y\n",
    "```\n",
    "```\n",
    "CPU times: user 19.6 s, sys: 160 ms, total: 19.8 s\n",
    "Wall time: 19.7 s\n",
    "```\n",
    "### Dask Array\n",
    "```python\n",
    "import dask.array as da\n",
    "\n",
    "%%time\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "y = x.mean(axis=0)[::100] \n",
    "y.compute() \n",
    "```\n",
    "```\n",
    "CPU times: user 29.4 s, sys: 1.07 s, total: 30.5 s\n",
    "Wall time: 4.01 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dask.dataframe\n",
    "\n",
    "\n",
    "* Out-of-core, blocked, parallel df\n",
    "* copies the Pandas API\n",
    "\n",
    "![dask-df](img/dask-df.PNG)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dask.bag\n",
    "\n",
    "* parallelizes across large collections of generic Python objects\n",
    "* dealing semi-structured like JSON blobs or log files\n",
    "* mimics iterators, Toolz and PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "` dask.bag = map, filter, toolz + multiprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# dask.bag creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### from seq\n",
    "```python\n",
    "import dask.bag as db\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### from files\n",
    "```python \n",
    "import os\n",
    "b = db.from_filenames(os.path.join('data', 'accounts.*.json.gz'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### from s3\n",
    "```python\n",
    "### Requires `boto` library\n",
    "b = db.from_s3('s3://nyqpug/tips.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# dask.bag storing\n",
    "\n",
    "#### In Memory\n",
    "` result = b.compute()`\n",
    "#### To Textfiles\n",
    "` b.to_textfiles('/path/to/data/*.json.gz')`\n",
    "#### To DataFrames\n",
    "``` \n",
    "df = b.to_dataframe()\n",
    "df.compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dask graph\n",
    "* Dictionary of (`name: task`)\n",
    "* Tasks are tuples of (`func, args..`) (lispy syntax)\n",
    "* Args can be names, values or tasks\n",
    "\n",
    "![dask-graph](img/dask-graph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### graph example\n",
    "\n",
    "```python\n",
    "from dask.dot import dot_graph\n",
    "def inc(i):\n",
    "    return i + 1\n",
    "\n",
    "def mul(c, d):\n",
    "    return c*d\n",
    "\n",
    "\n",
    "d= {'a': 1,\n",
    "    'b': 2,\n",
    "    'x': (inc, \"a\"),\n",
    "    'y': (inc, \"b\"),\n",
    "    'z': (mul, \"x\", \"y\")\n",
    "    }\n",
    "    \n",
    "dot_graph(dsk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![dask-graph2](img/dask-graph2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dask.shedulers\n",
    "\n",
    "\n",
    "#### Single machine scheduler\n",
    "\n",
    "* Local Threads\n",
    "\n",
    "```python\n",
    "import dask\n",
    "dask.set_options(get=dask.threaded.get)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Local Processes\n",
    "\n",
    "  - executes with local `multiprocessing.Pool`\n",
    "  - default scheduler for dask bag\n",
    "  \n",
    "```python\n",
    "import dask.multiprocessing\n",
    "dask.get_options(get=dask.multiprocessing.get)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# dask.schedulers - distributed\n",
    "\n",
    "#### local\n",
    "```python\n",
    "from dask.distributed import Client, progress\n",
    "client = Client(processes=False, threads_per_worker=4, n_workers=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### distributed\n",
    "\n",
    "* more sophisticated, but more features\n",
    "* manual setup\n",
    "  - `dask-sheduler` and `dask-worker`\n",
    "* SSH\n",
    "* Kubernetes\n",
    "* HPC (SLURM, SGE, PBS, MPI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## dask.distributed - HPC\n",
    "\n",
    "![dask-mpi](img/dask-mpi.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# takeaways\n",
    "* Python can still handle large datasets using blocked algo\n",
    "\n",
    "* Dask collections form task graphs expressing these algprithms\n",
    "\n",
    "* Dask schedulers execute these graphs in parallel\n",
    "\n",
    "* Dask graphs can be directly created for custom pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## parallelism norms \n",
    " \n",
    " **multi-process**, not multi-thread\n",
    " \n",
    " **multi-node**, not multi-core\n",
    " \n",
    " **message-passing**, not shared memory\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### **_Frameworks_**\n",
    " * futures/joblib\n",
    " * dask (data intensive tasks)\n",
    "     - computer/multicore/node/cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# on HPC\n",
    "\n",
    "Normal Python: Single processor \n",
    "\n",
    "parallel8/12/24 (Single Node)\n",
    "* python cannot make use of thos 8/12/24 processors\n",
    "* _mulitprocessing, futures, joblib_ \n",
    "* #PBS -l select=**1**:ncpus=24:mpiprocs=24:mem=160GB\n",
    "\n",
    "parallel8/12/24 (Multi Node)\n",
    "* distributed parallelism\n",
    "* _MPI4py, Dask_\n",
    "* #PBS -l select=**2**:ncpus=24:mpiprocs=24:mem=160GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# new on the plate\n",
    "\n",
    "* Bioinformatics services on HPC\n",
    "  - nextflow pipelines (RNAseq/other NGS)\n",
    "  - Database services\n",
    "\n",
    "\n",
    "* cloud services\n",
    "  - AWS\n",
    "  - Any Deep learning/ Other GPU jobs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "* http://sebastianraschka.com/Articles/2014_multiprocessing.html\n",
    "* http://masnun.com/2016/03/29/python-a-quick-introduction-to-the-concurrent-futures-module.html\n",
    "* http://pydata.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![thanks](img/thanks.PNG)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
